{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "stdout = sys.stdout\n",
    "reload(sys)\n",
    "sys.stdout = stdout\n",
    "\n",
    "import cPickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QACNN(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        # 输入\n",
    "        self.add_placeholders()\n",
    "        # [batch_size, sequence_size, embed_size]\n",
    "        q_embed, aplus_embed, aminus_embed = self.add_embeddings()\n",
    "        # [batch_size, sequence_size, hidden_size, 1]\n",
    "        self.h_q, self.h_ap, self.h_am = self.add_hl(q_embed, aplus_embed, aminus_embed)\n",
    "        # [batch_size, total_channels]\n",
    "        real_pool_q, real_pool_ap, real_pool_am = self.add_model(self.h_q, self.h_ap, self.h_am)\n",
    "        # [batch_size, 1]\n",
    "        self.q_ap_cosine, self.q_am_cosine = self.calc_cosine(real_pool_q, real_pool_ap, real_pool_am)\n",
    "        # 损失和精确度\n",
    "        self.total_loss, self.loss, self.accu = self.add_loss_op(self.q_ap_cosine, self.q_am_cosine)\n",
    "        # 训练节点\n",
    "        self.train_op = self.add_train_op(self.total_loss)\n",
    "\n",
    "\n",
    "    # 输入\n",
    "    def add_placeholders(self):\n",
    "        # 问题\n",
    "        self.q = tf.placeholder(np.int32,\n",
    "                shape=[None, self.config.max_q_length],\n",
    "                name='Question')\n",
    "        # 正向回答\n",
    "        self.aplus = tf.placeholder(np.int32,\n",
    "                shape=[None, self.config.max_a_length],\n",
    "                name='PosAns')\n",
    "        # 负向回答\n",
    "        self.aminus = tf.placeholder(np.int32,\n",
    "                shape=[None, self.config.max_a_length],\n",
    "                name='NegAns')\n",
    "        # drop_out\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "        self.batch_size = tf.shape(self.q)[0]\n",
    "\n",
    "    # word embeddings\n",
    "    def add_embeddings(self):\n",
    "        with tf.variable_scope('embedding'):\n",
    "            embeddings = tf.get_variable('embeddings', shape=[self.config.vocab_size, self.config.embedding_size], initializer=tf.uniform_unit_scaling_initializer())\n",
    "            q_embed = tf.nn.embedding_lookup(embeddings, self.q)\n",
    "            aplus_embed = tf.nn.embedding_lookup(embeddings, self.aplus)\n",
    "            aminus_embed = tf.nn.embedding_lookup(embeddings, self.aminus)\n",
    "            return q_embed, aplus_embed, aminus_embed\n",
    "\n",
    "    # Hidden Layer\n",
    "    def add_hl(self, q_embed, aplus_embed, aminus_embed):\n",
    "        with tf.variable_scope('HL'):\n",
    "            W = tf.get_variable('weights', shape=[self.config.embedding_size, self.config.hidden_size], initializer=tf.uniform_unit_scaling_initializer())\n",
    "            b = tf.get_variable('biases', initializer=tf.constant(0.1, shape=[self.config.hidden_size]))\n",
    "            h_q = tf.reshape(tf.nn.tanh(tf.matmul(tf.reshape(q_embed, [-1, self.config.embedding_size]), W)+b), [self.config.batch_size, self.config.max_q_length, -1])\n",
    "            h_ap = tf.reshape(tf.nn.tanh(tf.matmul(tf.reshape(aplus_embed, [-1, self.config.embedding_size]), W)+b), [self.config.batch_size, self.config.max_a_length, -1])\n",
    "            h_am = tf.reshape(tf.nn.tanh(tf.matmul(tf.reshape(aminus_embed, [-1, self.config.embedding_size]), W)+b), [self.config.batch_size, self.config.max_a_length, -1])\n",
    "            tf.add_to_collection('total_loss', 0.5*self.config.l2_reg_lambda*tf.nn.l2_loss(W))\n",
    "            # print 'h_q[shape]:', tf.shape(h_q)\n",
    "            # print 'h_ap[shape]:', tf.shape(h_ap)\n",
    "            # print 'h_am[shape]:', tf.shape(h_am)\n",
    "            return h_q, h_ap, h_am\n",
    "\n",
    "    # CNN层\n",
    "    def add_model(self, h_q, h_ap, h_am):\n",
    "        pool_q = list()\n",
    "        pool_ap = list()\n",
    "        pool_am = list()\n",
    "        h_q = tf.reshape(h_q, [-1, self.config.max_q_length, self.config.hidden_size, 1])\n",
    "        h_ap = tf.reshape(h_ap, [-1, self.config.max_a_length, self.config.hidden_size, 1])\n",
    "        h_am = tf.reshape(h_am, [-1, self.config.max_a_length, self.config.hidden_size, 1])\n",
    "        for i, filter_size in enumerate(self.config.filter_sizes):\n",
    "            with tf.variable_scope('filter{}'.format(filter_size)):\n",
    "                # filter的W和b\n",
    "                conv1_W = tf.get_variable('W', shape=[filter_size, self.config.hidden_size, 1, self.config.num_filters], initializer=tf.truncated_normal_initializer(.0, .1))\n",
    "                conv1_b = tf.get_variable('conv_b', initializer=tf.constant(0.1, shape=[self.config.num_filters]))\n",
    "                # pooling层的bias,Q和A分开\n",
    "                pool_qb = tf.get_variable('pool_qb', initializer=tf.constant(0.1, shape=[self.config.num_filters]))\n",
    "                pool_ab = tf.get_variable('pool_ab', initializer=tf.constant(0.1, shape=[self.config.num_filters]))\n",
    "                # 卷积\n",
    "                out_q = tf.nn.relu((tf.nn.conv2d(h_q, conv1_W, [1,1,1,1], padding='VALID')+conv1_b))\n",
    "                # 池化\n",
    "                out_q = tf.nn.max_pool(out_q, [1,self.config.max_q_length-filter_size+1,1,1], [1,1,1,1], padding='VALID')\n",
    "                out_q = tf.nn.tanh(out_q+pool_qb)\n",
    "                pool_q.append(out_q)\n",
    "\n",
    "                out_ap = tf.nn.relu((tf.nn.conv2d(h_ap, conv1_W, [1,1,1,1], padding='VALID')+conv1_b))\n",
    "                out_ap = tf.nn.max_pool(out_ap, [1,self.config.max_a_length-filter_size+1,1,1], [1,1,1,1], padding='VALID')\n",
    "                out_ap = tf.nn.tanh(out_ap+pool_ab)\n",
    "                pool_ap.append(out_ap)\n",
    "\n",
    "                out_am = tf.nn.relu((tf.nn.conv2d(h_am, conv1_W, [1,1,1,1], padding='VALID')+conv1_b))\n",
    "                out_am = tf.nn.max_pool(out_am, [1,self.config.max_a_length-filter_size+1,1,1], [1,1,1,1], padding='VALID')\n",
    "                out_am = tf.nn.tanh(out_am+pool_ab)\n",
    "                pool_am.append(out_am)\n",
    "\n",
    "                # 加入正则项\n",
    "                tf.add_to_collection('total_loss', 0.5*self.config.l2_reg_lambda*tf.nn.l2_loss(conv1_W))\n",
    "\n",
    "        total_channels = len(self.config.filter_sizes)*self.config.num_filters\n",
    "\n",
    "        real_pool_q = tf.reshape(tf.concat(3, pool_q), [-1, total_channels])\n",
    "        real_pool_ap = tf.reshape(tf.concat(3, pool_ap), [-1, total_channels])\n",
    "        real_pool_am = tf.reshape(tf.concat(3, pool_am), [-1, total_channels])\n",
    "        # print 'real_pool_q[shape]:', tf.shape(real_pool_q)\n",
    "        # print 'real_pool_ap[shape]:', tf.shape(real_pool_ap)\n",
    "        # print 'real_pool_am[shape]:', tf.shape(real_pool_am)\n",
    "\n",
    "        return real_pool_q, real_pool_ap, real_pool_am\n",
    "\n",
    "    # 计算cosine\n",
    "    def calc_cosine(self, real_pool_q, real_pool_ap, real_pool_am):\n",
    "        len_pool_q = tf.sqrt(tf.reduce_sum(tf.pow(real_pool_q, 2), [1]))\n",
    "        len_pool_ap = tf.sqrt(tf.reduce_sum(tf.pow(real_pool_ap, 2), [1]))\n",
    "        len_pool_am = tf.sqrt(tf.reduce_sum(tf.pow(real_pool_am, 2), [1]))\n",
    "        # print 'len_pool_q[shape]:', tf.shape(len_pool_q)\n",
    "        # print 'len_pool_ap[shape]:', tf.shape(len_pool_ap)\n",
    "        # print 'len_pool_am[shape]:', tf.shape(len_pool_am)\n",
    "\n",
    "        q_ap_cosine = tf.div(tf.reduce_sum(tf.mul(real_pool_q, real_pool_ap), [1]), tf.mul(len_pool_q, len_pool_ap))\n",
    "        q_am_cosine = tf.div(tf.reduce_sum(tf.mul(real_pool_q, real_pool_am), [1]), tf.mul(len_pool_q, len_pool_am))\n",
    "\n",
    "        return q_ap_cosine, q_am_cosine\n",
    "\n",
    "    # 损失节点\n",
    "    def add_loss_op(self, q_ap_cosine, q_am_cosine):\n",
    "        margin = tf.constant(self.config.m, shape=[self.batch_size], dtype=tf.float32)\n",
    "        # 0常量\n",
    "        zero = tf.constant(0., shape=[self.batch_size], dtype=tf.float32)\n",
    "        l = tf.maximum(zero, tf.add(tf.sub(margin, q_ap_cosine), q_am_cosine))\n",
    "        loss = tf.reduce_sum(l)\n",
    "        tf.add_to_collection('total_loss', loss)\n",
    "        total_loss = tf.add_n(tf.get_collection('total_loss'))\n",
    "        accu = tf.reduce_mean(tf.cast(tf.equal(zero, l), tf.float32))\n",
    "        # print 'q_am_cosine[shape]:', tf.shape(q_am_cosine)\n",
    "        # print 'q_ap_cosine[shape]:', tf.shape(q_ap_cosine)\n",
    "        # print 'loss[shape]:', tf.shape(loss)\n",
    "        # print 'accu[shape]:', tf.shape(accu)\n",
    "        return total_loss, loss, accu\n",
    "\n",
    "    # 训练节点\n",
    "    def add_train_op(self, loss):\n",
    "        with tf.name_scope('train_op'):\n",
    "            # 记录训练步骤\n",
    "            self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            opt = tf.train.AdamOptimizer(self.config.lr)\n",
    "            train_op = opt.minimize(loss, self.global_step)\n",
    "            return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self, vocab_size):\n",
    "        # 输入问题(句子)长度\n",
    "        self.max_q_length = 200\n",
    "        # 输入答案长度\n",
    "        self.max_a_length = 200\n",
    "        # 循环数\n",
    "        self.num_epochs = 100000\n",
    "        # batch大小\n",
    "        self.batch_size = 100\n",
    "        # 词表大小\n",
    "        self.vocab_size = vocab_size\n",
    "        # 词向量大小\n",
    "        self.embedding_size = 100\n",
    "        # 不同类型的filter,相当于1-gram,2-gram,3-gram和5-gram\n",
    "        self.filter_sizes = [1, 2, 3, 5]\n",
    "        # 隐层大小\n",
    "        self.hidden_size = 80\n",
    "        # 每种filter的数量\n",
    "        self.num_filters = 512\n",
    "        # L2正则化,未用,没啥效果\n",
    "        # 论文里给的是0.0001\n",
    "        self.l2_reg_lambda = 0.\n",
    "        # 弃权,未用,没啥效果\n",
    "        self.keep_prob = 1.0\n",
    "        # 学习率\n",
    "        # 论文里给的是0.01\n",
    "        self.lr = 0.01\n",
    "        # margin\n",
    "        # 论文里给的是0.009\n",
    "        self.m = 0.05\n",
    "        # 设定GPU的性质,允许将不能在GPU上处理的部分放到CPU\n",
    "        # 设置log打印\n",
    "        self.cf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "        # 只占用20%的GPU内存\n",
    "        self.cf.gpu_options.per_process_gpu_memory_fraction = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = '../data/WikiQA/raw'\n",
    "processed_data_path = '../data/WikiQA/processed'\n",
    "\n",
    "max_q_length = 40\n",
    "max_a_length = 40\n",
    "\n",
    "def padding(data, max_len):\n",
    "    return tf.keras.preprocessing.sequence.pad_sequences(data, max_len, padding='post', truncating='post')\n",
    "\n",
    "\n",
    "class Iterator(object):\n",
    "    \"\"\"\n",
    "    数据迭代器\n",
    "    \"\"\"\n",
    "    def __init__(self, x):\n",
    "        self.x = np.asarray(x)\n",
    "        self.sample_num = self.x.shape[0]\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        # produce X, Y_out, Y_in, X_len, Y_in_len, Y_out_len\n",
    "        l = np.random.randint(0, self.sample_num - batch_size + 1)\n",
    "        r = l + batch_size\n",
    "        x_part = self.x[l:r]\n",
    "        return x_part\n",
    "\n",
    "    def next(self, batch_size, shuffle=True):\n",
    "        np.random.shuffle(self.x)\n",
    "        l = 0\n",
    "        while l < self.sample_num:\n",
    "            r = min(l + batch_size, self.sample_num)\n",
    "            batch_size = r - l\n",
    "            x_part = self.x[l:r]\n",
    "            yield x_part\n",
    "\n",
    "\n",
    "with open(os.path.join(processed_data_path, 'pairwise_corpus.pkl'), 'r') as fr:\n",
    "    train_corpus, val_corpus, test_corpus = pkl.load(fr)\n",
    "    \n",
    "with open(os.path.join(processed_data_path, 'vocab.pkl'), 'r') as fr:\n",
    "    word2id, id2word = pkl.load(fr)\n",
    "    \n",
    "train_q, train_ap, train_an = zip(*train_corpus)\n",
    "\n",
    "train_q = padding(train_q, max_q_length)\n",
    "train_ap = padding(train_ap, max_a_length)\n",
    "train_an = padding(train_an, max_a_length)  \n",
    "\n",
    "\n",
    "config = Config(len(word2id))\n",
    "config.max_q_length = max_q_length\n",
    "config.max_a_length = max_a_length\n",
    "\n",
    "train_corpus = zip(train_q, train_ap, train_an)\n",
    "\n",
    "iterator = Iterator(train_corpus)\n",
    "\n",
    "with tf.Session(config=config.cf) as sess:\n",
    "    model = QACNN(config)\n",
    "    \n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    for epoch in config.num_epochs:\n",
    "        count = 0\n",
    "        for batch_x in iterator.next(config.batch_size):\n",
    "            batch_q, batch_ap, batch_an = zip(*batch_x)\n",
    "            print(batch_q.shape)\n",
    "            print(batch_ap.shape)\n",
    "            print(batch_an.shape)\n",
    "            _, loss = sess.run([model.train_op, model.total_loss], \n",
    "                               feed_dict={model.q:batch_q, \n",
    "                                          model.aplus:batch_ap, \n",
    "                                          model.aminus:batch_an,\n",
    "                                          model.keep_prob:config.keep_prob})\n",
    "            count += 1\n",
    "            if count % 100 == 0:\n",
    "                print('Loss:{}'.format(loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
